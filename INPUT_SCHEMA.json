{
  "title": "Web Scraper Input",
  "type": "object",
  "description": "Use the following form to configure Web Scraper. The Start URLs and Page function are required and all other fields are optional. If you want to use Pseudo URLs, you also need to Use request queue. To learn more about the different options, click on their title or on the nearby question marks. For details about the Page function visit the actor's README in the ACTOR INFO tab. We also have a <a href=\"https://apify.com/docs/scraping/tutorial/introduction\" target=\"_blank\">step by step tutorial</a> to help you with the basics.\n\nIf you get stuck and it seems that you can no longer progress with the Web Scraper, try <a href=\"https://apify.com/docs/scraping/tutorial/puppeteer-scraper\" target=\"_blank\">Puppeteer Scraper</a>. It's a more powerful tool.",
  "schemaVersion": 1,
  "properties": {
      "startUrls": {
          "title": "Start URLs",
          "type": "array",
          "description": "URLs to start with",
          "prefill": [
              { "url": "https://apify.com" }
          ],
          "editor": "requestListSources"
      },
      "useRequestQueue": {
          "title": "Use request queue",
          "type": "boolean",
          "description": "Request queue enables recursive crawling and the use of Pseudo-URLs, Link selector and <code>context.enqueueRequest()</code>.",
          "default": true
      },
      "pseudoUrls": {
          "title": "Pseudo-URLs",
          "type": "array",
          "description": "Pseudo-URLs to match links in the page that you want to enqueue. Combine with Link selector to tell the scraper where to find links. Omitting the Pseudo-URLs will cause the scraper to enqueue all links matched by the Link selector.",
          "editor": "pseudoUrls",
          "default": [],
          "prefill": [
              {
                  "purl": "https://apify.com[(/[\\w-]+)?]"
              }
          ]
      },
      "linkSelector": {
          "title": "Link selector",
          "type": "string",
          "description": "CSS selector matching elements with 'href' attributes that should be enqueued. To enqueue urls from <code><div class=\"my-class\" href=...></code> tags, you would enter <strong>div.my-class</strong>. Leave empty to ignore all links.",
          "editor": "textfield",
          "prefill": "a"
      },
      "keepUrlFragments": {
          "title": "Keep URL fragments",
          "type": "boolean",
          "description": "URL fragments (the parts of URL after a <code>#</code>) are not considered when the scraper determines whether a URL has already been visited. This means that when adding URLs such as <code>https://example.com/#foo</code> and <code>https://example.com/#bar</code>, only the first will be visited. Turn this option on to tell the scraper to visit both.",
          "default": false
      },
      "pageFunction": {
          "title": "Page function",
          "type": "string",
          "description": "Function executed for each request",
          "prefill": "async function pageFunction(context) {\n    // See README for context properties. If the syntax is unfamiliar see the link\n    // https://javascript.info/destructuring-assignment#object-destructuring\n    const { request, log, jQuery } = context;\n\n    // To be able to use jQuery as $, one needs save it into a variable\n    // and select the inject jQuery option. We've selected it for you.\n    const $ = jQuery;\n    const title = $('title').text();\n\n    // This is yet another new feature of Javascript called template strings.\n    // https://javascript.info/string#quotes\n    log.info(`URL: ${request.url} TITLE: ${title}`);\n\n    // To save data just return an object with the requested properties.\n    return {\n        url: request.url,\n        title\n    };\n}",
          "editor": "javascript"
      },
      "proxyConfiguration": {
          "title": "Proxy configuration",
          "type": "object",
          "description": "Choose to use no proxy, Apify Proxy, or provide custom proxy URLs.",
          "prefill": { "useApifyProxy": false },
          "default": {},
          "editor": "proxy"
      },
      "debugLog": {
          "title": "Debug log",
          "type": "boolean",
          "description": "Debug messages will be included in the log. Use <code>context.log.debug('message')</code> to log your own debug messages.",
          "default": false,
          "groupCaption": "Options",
          "groupDescription": "Scraper settings"
      },
      "XML": {
          "title": "Process as RSS / XML",
          "type": "boolean",
          "description": "If true if cheerio is parsing HTML or XML",
          "default": false
      },        
      "maxRequestRetries": {
          "title": "Max request retries",
          "type": "integer",
          "description": "Maximum number of times the request for the page will be retried in case of an error. Setting it to 0 means that the request will be attempted once and will not be retried if it fails.",
          "minimum": 0,
          "default": 3,
          "unit": "retries"
      },
      "maxPagesPerCrawl": {
          "title": "Max pages per run",
          "type": "integer",
          "description": "Maximum number of pages that the scraper will open. 0 means unlimited.",
          "minimum": 0,
          "default": 0,
          "unit": "pages"
      },
      "maxResultsPerCrawl": {
          "title": "Max result records",
          "type": "integer",
          "description": "Maximum number of results that will be saved to dataset. The scraper will terminate afterwards. 0 means unlimited.",
          "minimum": 0,
          "default": 0,
          "unit": "results"
      },
      "maxCrawlingDepth": {
          "title": "Max crawling depth",
          "type": "integer",
          "description": "Defines how many links away from the StartURLs will the scraper descend. 0 means unlimited.",
          "minimum": 0,
          "default": 0
      },
      "maxConcurrency": {
          "title": "Max concurrency",
          "type": "integer",
          "description": "Defines how many pages can be processed by the scraper in parallel. The scraper automatically increases and decreases concurrency based on available system resources. Use this option to set a hard limit.",
          "minimum": 1,
          "default": 50
      },
      "handleRequestTimeoutSecs": {
          "title": "handleRequestFunction timeout",
          "type": "integer",
          "description": "Maximum time the scraper will wait for the handleRequestFunction to execute in seconds.",
          "minimum": 1,
          "default": 60,
          "maximum": 360,
          "unit": "secs"        
      },
      "customData": {
          "title": "Custom data",
          "type": "object",
          "description": "This object will be available on pageFunction's context as customData.",
          "default": {},
          "prefill": {},
          "editor": "json"
      }
  },
  "required": ["startUrls", "pageFunction"]
}
